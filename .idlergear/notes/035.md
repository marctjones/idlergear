---
id: 35
created: '2026-01-11T05:27:02.026583Z'
tags:
- design
- explore
- tests
---
# Design: Test Awareness for IdlerGear

## The Problem

When developing with AI assistants:
1. **Tests get forgotten** - AI writes code but doesn't write/run tests
2. **Test state is unclear** - "Did tests pass last time? Which ones failed?"
3. **Coverage blind spots** - New code added without corresponding tests
4. **Re-running overhead** - AI runs full test suite repeatedly to check state

## User Stories

1. "What's the test status?" - Quick summary of last test run
2. "Does this file have tests?" - Check coverage before making changes
3. "Run the tests for what I just changed" - Smart test selection
4. "Remind me to write tests" - Prompt when new code lacks tests

## Proposed Solution: `idlergear test`

### Core Commands

```bash
# Detect test framework and configuration
idlergear test detect
# Output: pytest (tests/), 766 test files found

# Show last test run status (cached)
idlergear test status
# Output: 
#   Last run: 2 minutes ago
#   Result: 759 passed, 0 failed
#   Duration: 4m 5s

# Run tests and cache results
idlergear test run
idlergear test run --changed  # Only tests for changed files
idlergear test run --failed   # Re-run failed tests

# Check if file/module has tests
idlergear test coverage src/idlergear/doctor.py
# Output: 
#   tests/test_doctor.py - 32 tests
#   Coverage: functions covered

# List tests for a file
idlergear test list src/idlergear/doctor.py
# Output:
#   test_doctor.py::TestCheckStatus::test_status_values
#   test_doctor.py::TestCheckResult::test_create_simple_result
#   ...
```

### MCP Tools

```python
idlergear_test_status()      # Last run summary
idlergear_test_run()         # Run tests, return results
idlergear_test_coverage(path) # Check if file has tests
idlergear_test_detect()      # Detect framework
```

### Framework Detection

Support common frameworks:

| Language | Frameworks | Test Discovery |
|----------|------------|----------------|
| Python | pytest, unittest, nose | `pytest --collect-only`, `tests/`, `*_test.py` |
| Rust | cargo test | `cargo test --no-run`, `#[test]` |
| .NET | dotnet test, xUnit, NUnit | `*.Tests.csproj`, `*Tests.cs` |
| JavaScript | jest, mocha, vitest | `*.test.js`, `*.spec.js`, `__tests__/` |
| Go | go test | `*_test.go` |
| Ruby | rspec, minitest | `spec/`, `test/` |

### Test Result Storage

Store in `.idlergear/tests/`:

```
.idlergear/tests/
├── config.json          # Detected framework, test command
├── last-run.json        # Most recent run results
├── history/             # Historical runs (optional)
│   └── 2026-01-11T04:30:00.json
└── coverage-map.json    # File -> test file mapping
```

### Integration with Existing Features

1. **Runs Integration**
   - `idlergear test run` creates a Run entry
   - Test results stored as structured metadata on the run
   
2. **Watch Mode Integration**
   - Watch mode can detect "tests not run after code change"
   - Suggest: "You modified doctor.py but haven't run tests"

3. **Task Integration**
   - `idlergear task create "..." --needs-tests`
   - Track which tasks have associated test coverage

4. **Doctor Integration**
   - Doctor can check: "Tests haven't been run in 24 hours"
   - Doctor can check: "New files without test coverage"

### Hook Integration

Pre-commit or post-tool-use hooks could:
- Warn if modifying code without running tests
- Remind to add tests for new files
- Block commits if tests failing

### Smart Test Selection

For `idlergear test run --changed`:
1. Get list of changed files from git
2. Map to corresponding test files
3. Run only those tests
4. Much faster feedback loop

### Output Format

```json
{
  "framework": "pytest",
  "last_run": "2026-01-11T04:30:00Z",
  "duration_seconds": 245,
  "total": 766,
  "passed": 766,
  "failed": 0,
  "skipped": 0,
  "errors": 0,
  "failed_tests": [],
  "coverage_percent": null
}
```

## Implementation Phases

### Phase 1: Status & Detection (MVP)
- `idlergear test detect` - Find framework
- `idlergear test status` - Show cached last run
- `idlergear test run` - Run tests, parse output, cache results
- Support: pytest, cargo test, dotnet test

### Phase 2: Coverage Mapping
- `idlergear test coverage <file>` - Check if file has tests
- `idlergear test list <file>` - List tests for a file
- Build file -> test file mapping

### Phase 3: Smart Selection
- `idlergear test run --changed` - Run tests for changed files
- `idlergear test run --failed` - Re-run failed tests

### Phase 4: Integration
- Watch mode integration (suggest running tests)
- Doctor integration (check test health)
- Task integration (--needs-tests flag)

## Questions to Resolve

1. Should test results be a new knowledge type or extend Runs?
2. How much test output to store? (full log vs summary)
3. Should we parse coverage reports (lcov, cobertura)?
4. How to handle monorepos with multiple test suites?

## Alignment with Vision

This fits IdlerGear's mission:
- **Structured knowledge** - Test state as queryable data
- **Cross-session persistence** - "What failed last time?"
- **AI-observable** - MCP tools for test status
- **Command-based API** - Same interface regardless of framework

It does NOT:
- Replace test frameworks (just wraps/observes them)
- Become a CI/CD system
- Manage test infrastructure
