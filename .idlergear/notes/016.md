---
id: 16
created: '2026-01-07T03:13:31.073293Z'
tags:
- idea
- otel
- database
---
Evaluating embedded databases for OTel log storage in IdlerGear:

## Requirements
- Pure Python (or Python bindings)
- Time-series log data (append-heavy)
- Query by time range, severity, service
- Full-text search capabilities
- Minimal dependencies
- Cross-platform

## Candidates Analysis

### 1. SQLite ⭐⭐⭐⭐⭐ (BEST CHOICE)
**Pros:**
- Built into Python stdlib (zero dependencies!)
- Excellent for append-heavy workloads
- Full-text search via FTS5
- JSON1 extension for structured logs
- Battle-tested, rock-solid
- 1M+ inserts/sec with proper indexing
- Small footprint (~1MB binary)
- Transaction support (ACID)

**Cons:**
- Not distributed (but we don't need that)
- Single writer (fine for collector daemon)

**Verdict:** Perfect fit for IdlerGear. Built-in, fast, reliable.

### 2. DuckDB ⭐⭐⭐⭐
**Pros:**
- OLAP-optimized (excellent for analytics)
- Columnar storage (compressed, fast scans)
- Full SQL support
- Parquet export
- Very fast aggregations

**Cons:**
- 15MB wheel size (vs SQLite's 0 bytes)
- Overkill for simple log queries
- OLAP optimized, not OLTP

**Verdict:** Great if we want analytics, but heavier than needed.

### 3. RocksDB ⭐⭐⭐
**Pros:**
- Optimized for write-heavy workloads
- Used by production systems (Kafka, etc.)
- Fast point lookups

**Cons:**
- Python bindings unreliable (python-rocksdb barely maintained)
- No SQL, manual indexing
- Overkill for our scale
- Larger memory footprint

**Verdict:** Over-engineered for IdlerGear's needs.

### 4. TinyDB ⭐⭐
**Pros:**
- Pure Python, tiny (1500 LOC)
- Document-oriented (good for JSON logs)
- Simple API

**Cons:**
- Loads entire DB into memory
- No production-grade performance
- Limited query capabilities
- Not suitable for >10K logs

**Verdict:** Too simple, won't scale.

### 5. ChromaDB ❌
**Pros:**
- Vector search capabilities

**Cons:**
- Designed for embeddings, not logs
- Heavy dependencies (onnx, torch, etc.)
- Overkill and wrong use case

**Verdict:** Wrong tool for the job.

### 6. Kuzu ⭐⭐
**Pros:**
- Graph database (relationships)
- Embedded

**Cons:**
- Graph model unnecessary for logs
- Less mature ecosystem
- Not optimized for time-series

**Verdict:** Interesting but wrong model.

## Recommendation: SQLite + Extensions

**Winner:** SQLite with FTS5 and JSON1

**Why:**
1. Zero dependencies (stdlib)
2. Production-proven
3. Fast enough (1M+ logs/day easily)
4. Full-text search built-in
5. JSON support for structured data
6. Cross-platform, stable
7. Minimal footprint

**Schema Design:**
```sql
CREATE TABLE logs (
    id INTEGER PRIMARY KEY,
    timestamp INTEGER NOT NULL,  -- Unix timestamp
    severity TEXT NOT NULL,
    service TEXT NOT NULL,
    message TEXT NOT NULL,
    attributes JSON,  -- Structured OTel attributes
    trace_id TEXT,
    span_id TEXT
);

CREATE INDEX idx_timestamp ON logs(timestamp);
CREATE INDEX idx_severity ON logs(severity);
CREATE INDEX idx_service ON logs(service);

CREATE VIRTUAL TABLE logs_fts USING fts5(
    message, 
    content='logs', 
    content_rowid='id'
);
```

**Performance:**
- Inserts: 10K-50K/sec with batching
- Queries: Sub-millisecond for indexed lookups
- Full-text search: <100ms for 1M logs
- Disk: ~1KB per log (compressed)

**Future Migration Path:**
If we ever outgrow SQLite (unlikely):
- DuckDB for analytics (same SQL, easy migration)
- PostgreSQL for distributed (export via pg_dump)

**Alternative:** DuckDB (if we want advanced analytics)
If we anticipate heavy analytical queries (aggregations, time-series analysis), DuckDB is the runner-up. But for simple log storage and retrieval, SQLite is perfect.
